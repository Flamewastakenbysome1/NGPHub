from flask import Flask, request, jsonify
from flask_cors import CORS
import requests
from datetime import datetime
import json

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})  # Allow all origins for testing

# Ollama Configuration
OLLAMA_URL = "http://localhost:11434"
MODEL_NAME = "gemma3:12b"

# Simple rate limiting (in-memory, resets on restart)
user_requests = {}
MAX_REQUESTS_PER_HOUR = 50  # Way higher since it's local and free!

def check_rate_limit(user_id):
    """Simple rate limiting"""
    now = datetime.now()
    if user_id not in user_requests:
        user_requests[user_id] = []
    
    # Remove requests older than 1 hour
    user_requests[user_id] = [t for t in user_requests[user_id] 
                               if (now - t).seconds < 3600]
    
    if len(user_requests[user_id]) >= MAX_REQUESTS_PER_HOUR:
        return False
    
    user_requests[user_id].append(now)
    return True

def call_ollama(message, conversation_history=[], system_prompt=None):
    """Call local Ollama model"""
    try:
        # Build messages array
        messages = []
        
        # Add system prompt if provided
        if system_prompt:
            messages.append({
                "role": "system",
                "content": system_prompt
            })
        
        # Add conversation history
        for msg in conversation_history:
            messages.append(msg)
        
        # Add current user message
        messages.append({
            "role": "user",
            "content": message
        })
        
        # Call Ollama API
        response = requests.post(
            f"{OLLAMA_URL}/api/chat",
            json={
                "model": MODEL_NAME,
                "messages": messages,
                "stream": False
            },
            timeout=120  # 2 min timeout for local processing
        )
        
        if response.status_code == 200:
            result = response.json()
            return result['message']['content']
        else:
            return f"Ollama Error: Status {response.status_code}"
            
    except requests.exceptions.ConnectionError:
        return "Error: Ollama server is not running. Please start Ollama."
    except requests.exceptions.Timeout:
        return "Error: Request timed out. The model might be processing a large response."
    except Exception as e:
        return f"Error: {str(e)}"

def nirvana_respond(message, history=[], user_id="anonymous"):
    """
    Nirvana 1-Turbo logic using local Ollama
    """
    
    # Nirvana's personality system prompt
    system_prompt = """You are Nirvana, an AI assistant created by NGP Studios. 
You're helpful, friendly, and a bit casual in your responses. 
You're knowledgeable about coding, game development, and tech.
Keep responses concise but informative."""
    
    # Call local model
    ai_response = call_ollama(message, history, system_prompt)
    
    return {
        "response": ai_response,
        "model": f"Nirvana 1-Turbo ({MODEL_NAME})",
        "timestamp": datetime.now().isoformat(),
        "local": True
    }

@app.route('/api/health', methods=['GET'])
def health_check():
    """Check if server is running"""
    try:
        # Check if Ollama is actually running
        ollama_check = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        ollama_status = "online" if ollama_check.status_code == 200 else "offline"
    except:
        ollama_status = "offline"
    
    return jsonify({
        "status": "online",
        "ollama_status": ollama_status,
        "version": "Nirvana 1-Turbo",
        "model": MODEL_NAME,
        "timestamp": datetime.now().isoformat()
    })

@app.route('/api/chat', methods=['POST'])
def chat():
    """Main chat endpoint"""
    try:
        data = request.json
        message = data.get('message', '')
        user_id = data.get('user_id', 'anonymous')
        history = data.get('history', [])
        
        if not message:
            return jsonify({"error": "No message provided"}), 400
        
        # Check rate limit
        if not check_rate_limit(user_id):
            return jsonify({
                "error": "Rate limit exceeded. Please wait before sending more messages.",
                "limit": MAX_REQUESTS_PER_HOUR
            }), 429
        
        # Get AI response
        result = nirvana_respond(message, history, user_id)
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/models', methods=['GET'])
def get_models():
    """List available Ollama models"""
    try:
        response = requests.get(f"{OLLAMA_URL}/api/tags")
        if response.status_code == 200:
            models = response.json().get('models', [])
            return jsonify({
                "models": [m['name'] for m in models],
                "current": MODEL_NAME
            })
        else:
            return jsonify({"error": "Could not fetch models"}), 500
    except:
        return jsonify({"error": "Ollama not running"}), 503

if __name__ == '__main__':
    print("üî• Nirvana 1-Turbo Server Starting...")
    print(f"üì° Using Ollama model: {MODEL_NAME}")
    print(f"üåê Server will run on http://localhost:5000")
    print(f"‚ö° Rate limit: {MAX_REQUESTS_PER_HOUR} requests/hour per user")
    print("\n‚ö†Ô∏è  Make sure Ollama is running: ollama serve")
    
    app.run(host='0.0.0.0', port=5000, debug=True)
