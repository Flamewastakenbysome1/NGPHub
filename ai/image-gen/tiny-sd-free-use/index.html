import os
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"
os.environ["HF_HUB_DISABLE_SYMLINKS"] = "1"
os.environ['XFORMERS_DISABLED'] = '1'

from flask import Flask, render_template, request, jsonify, send_file
from flask_cors import CORS
from diffusers import StableDiffusionPipeline
import torch
import io
import base64
from PIL import Image
import time
import threading

app = Flask(__name__)
CORS(app)  # Enable CORS for all routes

# Global pipeline variable
pipe = None
device = None
generation_lock = threading.Lock()  # Prevents concurrent model access

def load_model():
    global pipe, device
    print("Loading tiny-sd model...")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    
    pipe = StableDiffusionPipeline.from_pretrained(
        "segmind/tiny-sd",
        dtype=torch.float16 if device == "cuda" else torch.float32,
        safety_checker=None,
        requires_safety_checker=False
    )
    
    if device == "cuda":
        pipe = pipe.to("cuda")
        pipe.enable_attention_slicing(1)
        pipe.enable_vae_slicing()
        print("GPU mode enabled")
    else:
        pipe = pipe.to("cpu")
    
    print("Model loaded!\n")

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/generate', methods=['POST'])
def generate():
    try:
        data = request.json
        prompt = data.get('prompt', '')
        negative_prompt = data.get('negative_prompt', '')
        steps = int(data.get('steps', 20))
        guidance = float(data.get('guidance', 7.5))
        seed = data.get('seed', None)
        width = int(data.get('width', 512))
        height = int(data.get('height', 512))
        
        if seed == '' or seed is None:
            seed = None
        else:
            seed = int(seed)
        
        print(f"Generating: '{prompt}'")
        start_time = time.time()
        
        generator = None
        if seed is not None:
            generator = torch.Generator(device=device).manual_seed(seed)
        
        # Use lock to prevent concurrent model access
        with generation_lock:
            print(f"  [STARTED] Generation for prompt: {prompt[:50]}...")
            with torch.inference_mode():
                result = pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    num_inference_steps=steps,
                    guidance_scale=guidance,
                    height=height,
                    width=width,
                    generator=generator
                )
            print(f"  [COMPLETED] Generation finished")
        
        image = result.images[0]
        elapsed = time.time() - start_time
        
        # Convert to base64
        buffered = io.BytesIO()
        image.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode()
        
        print(f"Generated in {elapsed:.2f}s")
        
        return jsonify({
            'success': True,
            'image': f'data:image/png;base64,{img_str}',
            'time': f'{elapsed:.2f}s'
        })
        
    except Exception as e:
        print(f"Error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

if __name__ == '__main__':
    load_model()
    print("\nðŸš€ Starting server at http://localhost:5000")
    print("Press Ctrl+C to stop\n")
    # Enable threaded mode for handling multiple requests
    app.run(debug=False, host='0.0.0.0', port=5000, threaded=True)
